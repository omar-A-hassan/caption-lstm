{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViL-Cap: Image Captioning with Vision-LSTM\n",
    "This notebook trains the ViL-Cap model on COCO 2017 captions dataset.\n",
    "\n",
    "**Dataset**: COCO 2017 Captions (kaggle.com/datasets/awsaf49/coco-2017-dataset)\n",
    "\n",
    "**Architecture**:\n",
    "- Encoder: Vision-LSTM (ViL) - bidirectional mLSTM for visual features\n",
    "- Decoder: Causal mLSTM for caption generation (3 blocks)\n",
    "- Fusion: Simple add/merge (no cross-attention, following Bi-LSTM paper)\n",
    "\n",
    "**Training Notes**:\n",
    "- This is a simplified training loop for demonstration\n",
    "- For production: use mixed precision (bfloat16), torch.compile, better augmentations\n",
    "- Recommended: Start with pretrained ViL encoder for faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/NX-AI/vision-lstm\n",
    "%cd vision-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q einops transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset and collator\n",
    "from src.ksuit.datasets import CocoCaptionsDataset\n",
    "from src.ksuit.data.collators import CaptionCollator\n",
    "\n",
    "# Image transforms (resize to 224x224 for ViL)\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CocoCaptionsDataset(\n",
    "    root=\"/kaggle/input/coco-2017-dataset/coco2017\",\n",
    "    split=\"train\",\n",
    "    return_all_captions=False,  # Random caption per image\n",
    ")\n",
    "\n",
    "val_dataset = CocoCaptionsDataset(\n",
    "    root=\"/kaggle/input/coco-2017-dataset/coco2017\",\n",
    "    split=\"val\",\n",
    "    return_all_captions=True,  # All captions for evaluation\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} images\")\n",
    "print(f\"Val dataset: {len(val_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collator and dataloaders\n",
    "collator = CaptionCollator(transform=image_transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Val batches: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from caption_lstm.model import ViLCap, ViLCapConfig\n",
    "\n",
    "# Create config\n",
    "config = ViLCapConfig(\n",
    "    # Encoder (ViL-T configuration)\n",
    "    encoder_dim=192,\n",
    "    encoder_depth=24,\n",
    "    encoder_input_shape=(3, 224, 224),\n",
    "    encoder_patch_size=16,\n",
    "    encoder_pooling=\"bilateral_avg\",\n",
    "    encoder_drop_path_rate=0.0,\n",
    "    encoder_pretrained_path=None,  # Set to path if using pretrained encoder\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_dim=512,\n",
    "    decoder_num_blocks=3,\n",
    "    decoder_num_heads=4,\n",
    "    decoder_dropout=0.2,\n",
    "    max_caption_length=50,\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer_model=\"bert-base-uncased\",\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = ViLCap(config).to(device)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"Encoder parameters: {sum(p.numel() for p in model.encoder.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"Decoder parameters: {sum(p.numel() for p in model.decoder.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 5\n",
    "lr = 1e-4\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "total_updates = len(train_dataloader) * epochs\n",
    "warmup_updates = int(total_updates * 0.1)\n",
    "\n",
    "# Learning rate schedule (linear warmup + linear decay)\n",
    "lrs = torch.cat([\n",
    "    torch.linspace(0, lr, warmup_updates),\n",
    "    torch.linspace(lr, 0, total_updates - warmup_updates),\n",
    "])\n",
    "\n",
    "print(f\"Total updates: {total_updates}\")\n",
    "print(f\"Warmup updates: {warmup_updates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "update = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "pbar = tqdm(total=total_updates)\n",
    "pbar.set_description(\"train_loss: ????? val_loss: ?????\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        images = batch['images'].to(device)\n",
    "        captions = batch['captions']\n",
    "        \n",
    "        # Schedule learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lrs[update]\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(images, captions=captions, mode='train')\n",
    "        logits = output['logits']\n",
    "        target_ids = output['target_ids']\n",
    "        attention_mask = output['attention_mask']\n",
    "        \n",
    "        # Compute loss (cross entropy)\n",
    "        # Flatten for loss computation\n",
    "        logits_flat = logits.reshape(-1, logits.size(-1))\n",
    "        target_flat = target_ids.reshape(-1)\n",
    "        \n",
    "        # Compute loss only on non-padded tokens\n",
    "        mask_flat = attention_mask.reshape(-1)\n",
    "        loss = F.cross_entropy(logits_flat, target_flat, reduction='none')\n",
    "        loss = (loss * mask_flat).sum() / mask_flat.sum()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Logging\n",
    "        train_losses.append(loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "        update += 1\n",
    "        \n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"train_loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    num_val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            images = batch['images'].to(device)\n",
    "            # For validation, take first caption from each image's caption list\n",
    "            captions = [caps[0] if isinstance(caps, list) else caps for caps in batch['captions']]\n",
    "            \n",
    "            output = model(images, captions=captions, mode='train')\n",
    "            logits = output['logits']\n",
    "            target_ids = output['target_ids']\n",
    "            attention_mask = output['attention_mask']\n",
    "            \n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))\n",
    "            target_flat = target_ids.reshape(-1)\n",
    "            mask_flat = attention_mask.reshape(-1)\n",
    "            \n",
    "            loss = F.cross_entropy(logits_flat, target_flat, reduction='none')\n",
    "            loss = (loss * mask_flat).sum() / mask_flat.sum()\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            num_val_batches += 1\n",
    "            \n",
    "            # Limit validation batches for speed\n",
    "            if num_val_batches >= 100:\n",
    "                break\n",
    "    \n",
    "    val_loss /= num_val_batches\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} - Train Loss: {epoch_loss/len(train_dataloader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(range(len(train_losses)), train_losses)\n",
    "axes[0].set_xlabel('Updates')\n",
    "axes[0].set_ylabel('Train Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(range(len(val_losses)), val_losses, marker='o')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Val Loss')\n",
    "axes[1].set_title('Validation Loss')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Captions on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate captions for sample images\n",
    "model.eval()\n",
    "\n",
    "# Get a batch from validation set\n",
    "sample_batch = next(iter(val_dataloader))\n",
    "sample_images = sample_batch['images'][:8].to(device)\n",
    "sample_gt_captions = sample_batch['captions'][:8]\n",
    "\n",
    "# Generate captions\n",
    "with torch.no_grad():\n",
    "    generated_captions = model.generate_captions(sample_images, temperature=1.0)\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Unnormalize images for display\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "sample_images_display = sample_images.cpu() * std + mean\n",
    "\n",
    "for i in range(8):\n",
    "    axes[i].imshow(sample_images_display[i].permute(1, 2, 0).clip(0, 1))\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    # Get ground truth (first caption if list)\n",
    "    gt = sample_gt_captions[i][0] if isinstance(sample_gt_captions[i], list) else sample_gt_captions[i]\n",
    "    \n",
    "    axes[i].set_title(f\"Generated: {generated_captions[i]}\\n\\nGT: {gt}\", fontsize=8, wrap=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "torch.save({\n",
    "    'epoch': epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'config': config,\n",
    "}, 'vilcap_checkpoint.pth')\n",
    "\n",
    "print(\"Model saved to vilcap_checkpoint.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
